# Support Generic HTLC Interception

- Add interception with a bitfield that indicates the types that they
  want to intercept

## Review Round 1

### Drop lockorder comments on ChannelManager

- Removes a comment that has the lock order for ChannelManager

Q: Mentions "automated lockorder issue detection"
- We have a file called `debug_sync.rs` which wraps regular locks
  - Inner lock `StdRwLock` (alias of regular `RwLock`
  - Metadata about the locking order `LockMetadata` (1)

(1) `LockMetadata`
- `lock_idx`: we globally track a counter every time we create a lock
- `locked_before`: `Map<idx, LockMetadata>`
- We can basically check this hashmap back for our own index

-> That is very cool.

### Move HTLC holding into HTLC decode from forward_htlcs

- Async payments added logic to hold htlcs until a message arrives
- This is implemented in `forward_htlcs`, where we take a look at
  `should_hold_htlc`.
  - `forward_htlcs` is called to push HTLCs into our `forward_htlcs`
    map, from which it will be retrieved to start processing the
    outgoing HTLC forward.
- This commit moves the holding logic to `process_pending_update_add_htlcs`

Q: Where is this in the flow?
- `process_pending_htlc_forwards` is polled periodically to process
  our next set of HTLCs
  - `process_pending_update_add_htlcs`: decodes our received
    `update_add_htlcs`, checks we can forward them, then calls
    `forward_htlcs` to push into the map for outgoing processing
  - `process_forward_htlcs`: grabs htlcs out of `forward_htlcs`
    and adds them on the outgoing channel.

When we're in an async payments setting, we're only _ever_ going to
receive a HTLC we want to hold via an `UpdateAddHTLC` (if we were
the originator of the HTLC, we can just hold off on it ourselves).
So we can move our hold processing to `process_pending_update_add_htlcs`,
which is the place where we receive such HTLCs.

Previously (if should_hold_htlc):
- Generate a htlc intercept ID
- Add to our `pending_intercepted_htlcs`
  - Log if inserted
  - Fail back if duplicated

Now (if should hold htlc):
- Generate a htlc intercept ID
- Add to our `pending_intercepted_htlcs`:
  - Log if inserted
  - Push into `htlc_fails` map

Q: How does `fail_intercepted_htlc` handle the failure? Is it the same
   as the `htlc_fail` and `failure_type` that we do in the new code?
- `fail_intercepted_htlc` is able to grab the HTLC out of
  `pending_intercepted_htlcs`:
  - `reason`: `LocalHTLCFailureReason`
  - `destination`: `InvalidForward`
-> The reason has changed, but it kinda makes sense

Q: How has failure handling changed?

Previously:
- `fail_intercepted_htlc` pushes to `fail_intercept_forwards`
- `fail_htlc_backwards_internal` (assuming this is a forward):
  - Creates a failure type based on blinding status
  - Adds `HTLCForwardInfo` to `forward_htlcs`
  - Pushes a `HTLCHandlingFailed`

Now:
- Push into our `htlc_fails` map
- Create `HTLCForwardInfo` based on failure type
- Adds `HTLCForwardInfo` to `forward_htlcs`
- Pushes a `HTLCHandlingFailed`

[x] Pre-existing, but: Can this be DRY-ed up a bit? 
-> Yes it can, done in a commit that follows!

### Fix the HTLC failure reason reported when a peer is offline

- Simple fix of errors + docs.

### DRY HTLC failure paths in process_pending_update_add_htlcs

- Add a macro to DRY up the ugly error handling mentioned above

### Support generic HTLC interception

`channelmanager.rs`:

- `process_pending_update_add_htlcs`:
Previously:
- Validated `can_forward_htlc`
- Puts htlcs we need to hold into `pending_intercepted_htlcs`

Now:
- Validates `can_forward_htlc_intercepted` (1)
- Create closure for creating `to_pending_add` and `intercept_id`
  (`intercept_id` hashes on a critical path, so we only want to hit
    if required)
- Puts htlcs we need to hold into `pending_intercepted_htlcs`
- If `intercept_forward`:
  - Lookup the entry in `pending_intercepted_htlcs`
    - Generate an interception event for the HTLC `create_htlc_intercepted_event` (2)
    - Fail if can't create event
    - Push into `entry`
  - Fail if duplicated

This is the same as the logic that we used to have in `forward_htlcs`,
except that we have this new event creation.

(1) `can_forward_htlc` -> `can_forward_htlc_intercepted`
Previously:
- Checks `can_forward_htlc_to_outgoing_channel`

Now: 
- Checks `can_forward_htlc_to_outgoing_channel`, which now returns
  a bool, which we return alongside (1.1)

(1.1) `can_forward_htlc_to_outgoing_channel`
- Now calls `forward_needs_intercept` to get a bool (1.2)
- Only errors out on disabled if the channel is live
- Returns a boolean

Q: These things seem really co-mingled, can't we split them us and
   pass in the `intercepted` bool at an earlier stage?
- We could call `forward_needs_intercept` before
  `can_forward_htlc_to_outgoing_channel` both within the same closure

(1.2) `forward_needs_intercept`:
- We do need the `FundedChannel` to check our current status
- Called in a few places, might help with decoupling

(2) `create_htlc_intercepted_event`
- Fails if we don't have an `incoming_amt_msat` ([ ] this should be checked elsewhere)
- Fails if we don't have forward type HTLC
- Generates the event

Q: Could we just return this in the place where we first note that we
   want to generate the intercept? Rather than returning a bool and
   then creating an event with the unwraps later?
   - Is this always called with `can_forward_htlc_intercepted`?
     - `process_pending_udpate_add_htlcs`: yes
     - `handle_release_held_htlc`: no, called with `forward_needs_intercept`
     - `ChannelManagerReadArgs`: no, just called directly from event  


- `forward_htlcs`:
Previously:
- Has a `fail_intercepted_htlc` closure which would push into
  `fail_intercepted_forwards` (later all shoved into
  `fail_htlc_backwards_internal`)
- If not forwarding to a known SCID + valid intercept:
  - Add to map (fail if already there) + push event 
- Otherwise just push into `forward_htlcs`
- If `new_intercept_events` isn't empty, push out events

Now:
- Just push into `forward_htlcs`, simple

Q: Why did we check `incoming_amt_msat.is_some()`?
- Because our own payments also go through this add pipeline


`util/config.rs`:
- Adds a bitfield for interception types:
  - `ToInterceptSCIDs`: htlcs that are generated using
    `get_intercept_scid`, which allows specifically marking for
    intercept
  - `ToOfflinePrivateChannels`: wake up offline peers w/ private chans
  - `ToOnlinePrivateChannels`: private channels, allows all in combo
  - `ToPublicChannels`: any announced channel
  - `ToAllKnownScids`: all of our fake and real SCIDs 
  - `ToUnknownSCIDs`: to a scid that we don't know (not intercept/real)
  - `AllValidHTLCs`: *

[ ] Q: should we have validation that it only uses known types?

`pending_changelog/`:
Why aren't HTLCs from 0.2 intercepted?
- Is it because we now need to set the flag? No.
- Because we've moved processing to pending adds (by the time
   they're locked in, we now won't hit interception on restart).
   - Create HTLC in old software, add to forward_htlcs
   - Shut down to upgrade
   - Previously, would notify when processed out for forward_htlcs
   - Now, it's in process_pending_adds (or w/e) so we "miss" it

Various test changes:
- Just updating tests to use interception API, nothing major

Looking at the TODO about needing to check multiple namespaces, which
is also related to wanting to decouple validation + interception:
- Split up the interception check (which is code smelly branched anyway) 
