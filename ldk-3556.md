# Fail back before upstream claim [#3556](https://github.com/lightningdevkit/rust-lightning/pull/3556/commits/a78ea1ffe68cfa3f4b438bd7a1d52f2fe63d90b0)

- `PendingHtlcRouting` represents different types of htlcs that we can
  relay; the other variants already have the incoming cltv set because
  they're for receives and it's needed for final cltv delta check
- `HTLCPreviousHopData` stores the things that we need to remember about
  the htlc when we settle/fail it back (shared secret/blinding stuff
  etc).
  - This is persisted, so old forwards won't have it stored (option),
    added to the impl_writable_tlv_based macro to persist
  - Used in two places in the codebase:
    - ClaimableHTLC: a htlc terminating at our node, we could claim
    - HTLCSource: an outgoing htlc, if it was forwarded to us

Q: failed_back_htlc_ids is in-memory only, how does this behave after
   restart? Assuming that we'll hit the code that re-checks them and
   re-adds?

- Called when a block is connected
  - No action if channel force closed, spent elsewhere or we've just
    signed a commitment transaction and are waiting for our onchain
    handler to deal with it
- Get all the HTLCs on our *latest* commitment transaction (even if
  we have another one that's pending revocation)
- Push into queue 

[Q]: Do we call block_connected when we start up? If we don't we can hit
   a restart case where something needs to be failed back to prevent the
   force close but we don't until the next block is reached
   - That's probably fine; the force close is only going to happen on a
     connected block anyway?
   - It would only happen if:
     - We go down at incoming_expiry + latency grace period + 1
     - We come back up at `incoming_expiry` and then do two things at
       once:
       - Force close the channel
       - Try to fail back this htlc
       -> Depending on which happens first, could force close when we
          could have just failed one htlc back?
       -> This is assuming that we close on `incoming_expiry` exactly,
          not sure that's the case
          [ ] Check what LATENCY_GRACE_PERIOD is used for (both?)


[ ]: update_monitor has a new failure case introduced but no test, is
  this a chance to contribute?

Orthogonal observations:
- PR removes a fuzz test, take a look at how that commit works
(historical_inbound_htlc_fulfills)
- Take another look at htlc flow, specifically how these pieces fit in

